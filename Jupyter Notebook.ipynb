{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qIFj9IyAU_v5","outputId":"276fa208-cd6d-4c79-fad0-faf90138bafc","executionInfo":{"status":"ok","timestamp":1736499110338,"user_tz":0,"elapsed":21357,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Libraries for data processing\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import random_split, DataLoader, WeightedRandomSampler, TensorDataset, ConcatDataset, Subset, Dataset\n","from torchvision import datasets, transforms, models\n","from sklearn.metrics import precision_score, classification_report, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from PIL import Image\n","\n","# Libraries for metrics and visualization\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","# Library for file handling\n","import os\n","from google.colab import drive"],"metadata":{"id":"9ziHomzxgTar","executionInfo":{"status":"ok","timestamp":1736499135415,"user_tz":0,"elapsed":20149,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6422ccc-dad7-4626-f078-e4c4e110f4e9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"code","source":["# Define dataset path\n","dataset_path = '/content/drive/My Drive/DS3'\n","\n","# Define data preprocessing transformations\n","transform = transforms.Compose([\n","    # Resize all images to 224x224\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","     # Normalize pixel values to [-1, 1]\n","    transforms.Normalize(mean=[0.5], std=[0.5])\n","])\n","\n","# Load the full dataset\n","full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n","print(f\"Loaded {len(full_dataset)} images from dataset.\")\n","\n","# Split the dataset into train, validation, and test sets\n","train_size = int(0.7 * len(full_dataset))  # 70% training data\n","val_size = len(full_dataset) - train_size  # 30% validation data\n","\n","\n","\n","generator = torch.Generator().manual_seed(42)  # Seed for reproducibility\n","train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n","#train_dataset, val_dataset = train_test_split(full_dataset, test_size=0.3, random_state=42)\n","\n","\n","# Apply the normalization to [-1, 1]\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values to [-1, 1]\n","])\n","\n","# Isolate dementia samples from train_dataset\n","train_targets = [full_dataset.targets[i] for i in train_dataset.indices]\n","train_targets = np.array(train_targets)\n","\n","dementia_class_idx = full_dataset.class_to_idx['Dementia']\n","dementia_indices = np.where(train_targets == dementia_class_idx)[0]\n","\n","current_dementia_count = len(dementia_indices)\n","\n","print(f\"Current Dementia count: {current_dementia_count}\")\n","print(f\"Train size (original): {len(train_dataset)}\")\n","print(f\"Validation size: {len(val_dataset)}\")\n","print(f\"\")\n","#print(f\"Augmentation needed: {augment_count}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6Q3SatPcRJf","executionInfo":{"status":"ok","timestamp":1736499181842,"user_tz":0,"elapsed":46428,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"outputId":"44cfa336-4405-4684-b2aa-930dc1721ad0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 15435 images from dataset.\n","Current Dementia count: 3835\n","Train size (original): 10804\n","Validation size: 4631\n","\n"]}]},{"cell_type":"code","source":["# Set number of workers based on the system\n","num_workers = 4 if torch.cuda.is_available() else 0\n","\n","# Create a DataLoader for the combined dataset\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=num_workers, pin_memory=True)\n","#test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=num_workers, pin_memory=True)"],"metadata":{"id":"41oGCObhoTvZ","executionInfo":{"status":"ok","timestamp":1736499389301,"user_tz":0,"elapsed":206,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Assuming `full_dataset` is your ImageFolder dataset\n","class_indices = full_dataset.class_to_idx\n","\n","# Get the index for the 'Dementia' class\n","demented_class_idx = class_indices['Dementia']\n","\n","# Print the class indices and the specific index for 'Dementia'\n","print(\"Class Indices Mapping:\", class_indices)\n","print(\"Index for 'Dementia':\", demented_class_idx)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bavdo0zS3HMN","executionInfo":{"status":"ok","timestamp":1736499390642,"user_tz":0,"elapsed":243,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"outputId":"443a2b91-408d-4080-a66f-f0f6095b051e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Class Indices Mapping: {'Dementia': 0, 'Non Demnted': 1}\n","Index for 'Dementia': 0\n"]}]},{"cell_type":"markdown","source":["removing augmetnration\n"],"metadata":{"id":"wqmnaxFchjf9"}},{"cell_type":"markdown","source":["Added batch data augmentation to make about 4000 samples for the demetia/demented class"],"metadata":{"id":"kElXBELQKTF6"}},{"cell_type":"code","source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"id":"ozFxH8ZE5uhX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736499401466,"user_tz":0,"elapsed":203,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"outputId":"1a3f6055-b195-4a52-c09e-3cad9ae681c1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# Load pre-trained ResNet50\n","model = models.resnet50(pretrained=True)\n","\n","# Modify the final fully connected layer for binary classification\n","model.fc = nn.Linear(in_features=2048, out_features=2)\n","\n","# Move the model to the device\n","model = model.to(device)\n","\n","# Define loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Initialize learning rate scheduler\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)"],"metadata":{"id":"se1FKTh34gbH","executionInfo":{"status":"ok","timestamp":1736499403332,"user_tz":0,"elapsed":820,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"96379d04-5a64-4d78-b117-5014fb759539"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Define checkpoint path\n","checkpoint_path = '/content/drive/My Drive/saved-model/model_checkpoint.pth'\n","\n","# Load checkpoint if it exists\n","if os.path.exists(checkpoint_path):\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    start_epoch = checkpoint['epoch']\n","    best_loss = checkpoint['loss']\n","    print(f\"Resumed training from epoch {start_epoch}, validation loss: {best_loss:.4f}\")\n","else:\n","    print(\"No checkpoint found, starting fresh.\")\n","    start_epoch = 0\n","    best_loss = float('inf')"],"metadata":{"id":"TBBK6kLgGvQq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736499406151,"user_tz":0,"elapsed":306,"user":{"displayName":"Belal Ahmed","userId":"15757959122185255818"}},"outputId":"cae9418d-c451-49c9-dc88-c380c33e2121"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["No checkpoint found, starting fresh.\n"]}]},{"cell_type":"code","source":["# Early stopping setup\n","patience = 3\n","epochs_since_improvement = 0\n","\n","# Initialize lists to store metrics for plotting after each epoch\n","train_losses, val_losses = [], []\n","train_accuracies, val_accuracies = [], []\n","epoch_times = []\n","\n","# Number of epochs\n","num_epochs = 5\n","checkpoint_interval = 5  # Save checkpoint every 5 epochs\n","\n","# Regular wieght loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","# Initialize best_loss before the loop\n","best_loss = float('inf')\n","\n","# Training Loop\n","for epoch in range(start_epoch, num_epochs):\n","    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n","\n","    epoch_start_time = time.time()\n","\n","    # Training phase\n","    model.train()\n","    running_loss, all_preds, all_labels = 0.0, [], []\n","\n","    for i, (inputs, labels) in enumerate(train_loader, 1):  # Start batch index from 1\n","      batch_start_time = time.time()\n","      inputs, labels = inputs.to(device), labels.to(device)\n","      optimizer.zero_grad()\n","\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)  # Weighted loss function\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","      _, preds = torch.max(outputs, 1)\n","      all_preds.extend(preds.cpu().numpy())\n","      all_labels.extend(labels.cpu().numpy())\n","\n","      # Print every 10 iterations\n","      if i % 10 == 0:\n","          batch_end_time = time.time()\n","          batch_time = batch_end_time - batch_start_time\n","          print(f\"Iteration {i}/{len(train_loader)} - Loss: {loss.item():.4f}, Batch Time: {batch_time:.4f} seconds\")\n","\n","    # Calculate training metrics after the loop ends\n","    avg_train_loss = running_loss / len(train_loader)\n","    train_accuracy = accuracy_score(all_labels, all_preds)\n","    train_losses.append(avg_train_loss)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Calculate the elapsed time for this epoch\n","    epoch_end_time = time.time()\n","    epoch_time = epoch_end_time - epoch_start_time\n","    epoch_times.append(epoch_time)\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss, val_preds, val_labels = 0.0, [], []  # Initialize val_loss here\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)  # Weighted loss function\n","\n","            val_loss += loss.item()\n","            _, preds = torch.max(outputs, 1)\n","            val_preds.extend(preds.cpu().numpy())\n","            val_labels.extend(labels.cpu().numpy())\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_accuracy = accuracy_score(val_labels, val_preds)\n","    val_losses.append(avg_val_loss)\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f\"\\nEpoch Summary [{epoch + 1}/{num_epochs}]\")\n","    print(f\"Training -> Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy * 100:.2f}%\")\n","    print(f\"Validation -> Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy * 100:.2f}%\")\n","    print(f\"Time taken for this epoch: {epoch_time:.2f} seconds\")\n","\n","    # Early stopping logic\n","    if avg_val_loss < best_loss:\n","        best_loss = avg_val_loss\n","        epochs_since_improvement = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","        print(\"Saved best model!\")\n","    else:\n","        epochs_since_improvement += 1\n","        if epochs_since_improvement >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","    # -------------------------\n","    # Plot Metrics After Each Epoch\n","    # -------------------------\n","    plt.figure(figsize=(15, 5))\n","\n","    # Loss Plot\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label=\"Training Loss\")\n","    plt.plot(val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Loss Over Epochs\")\n","    plt.legend()\n","\n","    # Accuracy Plot\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label=\"Training Accuracy\")\n","    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.title(\"Accuracy Over Epochs\")\n","    plt.legend()\n","\n","    plt.suptitle(f\"Metrics After Epoch {epoch + 1}\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Debugging: Check confusion matrix after each epoch\n","    print(\"\\nConfusion Matrix (Validation):\")\n","    print(confusion_matrix(val_labels, val_preds))\n","\n","    print(\"\\nClassification Report (Validation):\")\n","    print(classification_report(val_labels, val_preds, target_names=['Non Demented', 'Dementia']))\n","\n","    # Calculate Precision, Recall, F1-score\n","    precision = precision_score(val_labels, val_preds, average=None)  # None for class-wise precision\n","    recall = recall_score(val_labels, val_preds, average=None)  # None for class-wise recall\n","    f1 = f1_score(val_labels, val_preds, average=None)  # None for class-wise F1-score\n","\n","    # Print these metrics\n","    print(f\"Precision (Non Demented): {precision[1]:.4f}, Precision (Dementia): {precision[0]:.4f}\")\n","    print(f\"Recall (Non Demented): {recall[1]:.4f}, Recall (Dementia): {recall[0]:.4f}\")\n","    print(f\"F1-Score (Non Demented): {f1[1]:.4f}, F1-Score (Dementia): {f1[0]:.4f}\")\n","\n","torch.save(model.state_dict(), '/content/drive/My Drive/saved-model/final_model.pth')\n","print(\"Training complete. Final model saved as 'final_model.pth'.\")\n","\n"],"metadata":{"id":"VwA-UnzSHFDM"},"execution_count":null,"outputs":[]}]}